---
title: 'Session 2: Homework 1'
author: 'Study Group B9: Sherington AMARAPALA, Gianmaria BARTOCCIONI, Metavee LUANGTHAWORNKUL,
  Keshav MAHENDRA, Hadrien PISTRE, Cissie XU'
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: yes
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---


```{r setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggrepel)
library(reshape2)
library(readxl)
library(dplyr)
library(kableExtra)
library(knitr)
library(ggthemes)
library(lubridate)
library(fivethirtyeight)
library(here)
library(skimr)
library(janitor)
library(vroom)
library(tidyquant)
library(rvest) # to scrape wikipedia page
```



# Where Do People Drink The Most Beer, Wine And Spirits?

First we need to install the `fivethirtyeight` package and load the drinks dataset.

Back in 2014, [fivethiryeight.com](https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/) published an article on alchohol consumption in different countries. The data `drinks` is available as part of the `fivethirtyeight` package. Make sure you have installed the `fivethirtyeight` package before proceeding.

```{r load_alcohol_data, cache=TRUE}
library(fivethirtyeight)
alcohol_direct <- read_csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/alcohol-consumption/drinks.csv") 
```


**What are the variable types? Any missing values we should worry about?**

First we use glimpse and skimr to take a closer look at our data. We make two important observations: 

1. There are two variable types: character and numerical double.
2. Some countries seem to do not have official alcohol consumption data. The skimr function seems to indicate that the data is clean. 

```{r glimpse_skim_data, cache=TRUE}
glimpse(alcohol_direct)
skimr::skim(alcohol_direct)

#looking closer at countries with missing data or low alcohol consumption
alcohol_direct %>%
  filter(total_litres_of_pure_alcohol<1) %>%
  group_by(country) %>%
  summarize(beer_servings,spirit_servings,wine_servings,total_litres_of_pure_alcohol)%>%
  arrange(total_litres_of_pure_alcohol)
```


## Making Plots showing alcohol consumption by category

**Top 25 beer consuming countries**

```{r beer_plot, cache=TRUE}
# YOUR CODE GOES HERE

alcohol_direct %>% #We take the data set.
  slice_max(order_by = beer_servings, n=25) %>%  #We sort the number of beer servings for the first 25 countries.
  ggplot(aes(x = beer_servings, y = fct_reorder(country, beer_servings))) + #We plot the number of beer servings for the first 25 countries in descending order.
  geom_col()+ #We make a column graph.
  theme_bw()+ #We format it to make it more easily readable.
  labs(
    title = "Top 25 beer consuming countries",
    x = "Beer servings per capita",
    y = NULL
  )+
  
#Finally, we format the main title, the axes' titles and the axes' text to make the chart more easily readable.
theme(plot.title = element_text(size = 14, face = "bold", hjust=.5), 
      axis.text=element_text(size=10), 
      axis.title.x = element_text(size = 12, angle = 0)) +
  NULL
```



**Top 25 wine consuming countries**

```{r wine_plot, cache=TRUE}

# YOUR CODE GOES HERE

alcohol_direct %>% #We take the data set.
  slice_max(order_by = wine_servings, n=25) %>% #We sort the number of wine servings for the first 25 countries.
  ggplot(aes(x = wine_servings, y = fct_reorder(country, wine_servings))) + #We plot the number of wine servings for the first 25 countries in descending order.
  geom_col()+ #We make a column graph.
  theme_bw()+ #We format it to make it more easily readable.
  labs(
    title = "Top 25 wine consuming countries",
    x = "Wine servings per capita",
    y = NULL
  )+
  
#Finally, we format the main title, the axes' titles and the axes' text to make the chart more easily readable.
theme(plot.title = element_text(size = 14, face = "bold", hjust=.5), 
      axis.text=element_text(size=10), 
      axis.title.x = element_text(size = 12, angle = 0)) +
  NULL
```




**Top 25 spirit consuming countries**

```{r spirit_plot, cache=TRUE}
# YOUR CODE GOES HERE

alcohol_direct %>% #We take the data set
  slice_max(order_by = spirit_servings, n=25) %>% #We sort the number of spirit servings for the first 25 countries
  ggplot(aes(x = spirit_servings, y = fct_reorder(country, spirit_servings))) + #We plot the number of spirit servings for the first 25 countries in descending order.
  geom_col()+ #We make a column graph.
  theme_bw()+ #We format it to make it more easily readable.
  labs(
    title = "Top 25 spirit consuming countries",
    x = "Spirit servings per capita",
    y = NULL
  )+
  
#Finally, we format the main title, the axes' titles and the axes' text to make the chart more easily readable.
theme(plot.title = element_text(size = 14, face = "bold", hjust=.5), 
      axis.text=element_text(size=10), 
      axis.title.x = element_text(size = 12, angle = 0)) +
  NULL
```

**What we can infer from these plots**

From these graphs, we can see that consumption habits vary by country. Muslim countries logically do not have any alcohol consumption. Western European countries tend to consume more wine per capita, whereas Eastern European countries tend to consume more beer per capita. Spirit consumption per capita is more important in South American countries. These differences are probably mainly explained by cultural and geographical factors: the climate of countries such as Italy, France or Spain are more suitable for wine production, whereas the climate of Eastern European countries or Asian countries is more suitable to beer production. Spirit production is less dependent on climate conditions: this explains why spirits are consumed worldwide in countries which have very different climates (e.g. Grenada, Russia).

Some countries have missing data or report 0 alcohol consumption. These countries are primarily muslim countries, where alcohol consumption might be banned by law. While alcohol consumption is banned, it does not nessecarily mean that it does not exist, it just might happen behind closed doors. Therefore, this data might fail to capture the real level of alcohol consumption accurately in some of these countries. Additionally, in some muslim countries, alcohol consumption is legal for the non-muslim population, which could explain the low reported per capita level of alcohol consumption. 



---



# Analysis of movies- IMDB dataset

First we will load our subset sample of movies, taken from the 'Kaggle IMDB 5000 movie dataset':

```{r load_movies, warning=FALSE, message=FALSE, cache=TRUE}
movies <- read_csv("https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset")
```

**Cleaning and inspecting the data**

The first thing we must do before we start modelling is to clean and inspect the data.

**Are there any missing values (NAs)? Are all entries distinct or are there duplicate entries?**

The skimr function tells us there are no missing values and 54 duplicates.

```{r skim_movies, warning=FALSE, message=FALSE, cache=TRUE}
skimr::skim(movies) #We use the skim function on the data set to obtain data properties and highlight duplicate entries.
movies <- read_csv(here::here("data", "movies.csv"))
glimpse(movies)
```




## Creating tables to explore the data

**Table with the count of movies by genre, ranked in descending order**

```{r count_movies_genres, warning=FALSE, message=FALSE, cache=TRUE}

movies %>% #We take the relevant dataset.
  group_by(genre) %>% #We group the movies by genre.
  count(sort=TRUE) %>% #Count the number of movies in each genre and rank genres by the descending order of movies they contain.
  kable() %>% #We format the table to make it more easily readable.
  kable_styling()

```




**Table with the average gross earning and budget by genre**

Also calculating a variable `return_on_budget` which shows how many $ did a movie make at the box office for each $ of its budget. Grouped by genres using this `return_on_budget` as a ranking in descending order. Average gross and average budget is in $ Millions while return on budget is a multiple (x). 

```{r gross_and_budget_movies, warning=FALSE, message=FALSE, cache=TRUE}

movies %>% #We take the relevant dataset.
  group_by(genre) %>% #We group the movies by genre.
  summarize(average_gross = mean(gross)/1000000, average_budget = mean(budget)/1000000) %>% #We summarise two variables to obtain the mean gross and the mean budget of each movie genre, converted to millions.
  mutate(return_on_budget = average_gross/average_budget) %>% #We create a new variable, return on budget multiple, with the mutate function by dividing the average gross by the average budget.
  arrange(desc(return_on_budget)) %>% #We arrange the movie genres by descening order of return on budget.
  kable() %>% #Finally, we format the table to make it more easily readable.
  kable_styling()
```




**Table showing the top 15 directors who have created the highest gross revenue in the box office**

```{r director_gross_load_movies, warning=FALSE, message=FALSE, cache=TRUE}

movies %>% #We take the relevant dataset.
  group_by(director) %>% #We group the movies by director.
  summarise(total_gross = sum(gross)/1000000, mean=mean(gross)/1000000,sd=sd(gross)/1000000,median=median(gross)/1000000) %>% #We summarise four variables to obtain the total gross revenue, the mean gross revenue, the standard deviation of gross revenue and the median of gross revenue in millions USD, by movie directors.
  arrange(desc(total_gross)) %>% #We arrange the directors by descending order of the gross revenue their movies made.
  head(15) %>% #We only take the first 15 directors.
  kable() %>% #Finally, we format the table to make it more easily readable.
  kable_styling()

```




**Table and a histogram that describes and visually shows how ratings are distributed by genre**

```{r rating_distribution_load_movies, warning=FALSE, message=FALSE, cache=TRUE}

table_rating <- movies %>% #We take the relevant dataset and assign our code to a variable.
  group_by(genre) %>% #We group the movies by genre.
  summarise(mean=mean(rating),sd=sd(rating),median=median(rating),min=min(rating),max=max(rating)) %>% #We summarise five variables to obtain the mean rating, the rating standard deviation, the median rating, the minimum rating and the maximum rating by movie genre.
  arrange(desc(mean)) %>% #We arrange the movie genres by descending order of rating.
  kable() %>% #We format the table to make it more easily readable.
  kable_styling()
table_rating #We display our table.

histogram_rating <- movies %>% #We take the relevant dataset and assign our code to a variable.
  ggplot(aes(x = rating, fill = genre, colour = genre))+ #We plot the rating by movie genres and colour each movie genre to see the distribution of ratings by movie genres.
  geom_histogram(bins = 30)+ #We create a histogram with an easily readable band width.
  theme_bw()+ #We assign a relevant title and x-axis to our graph.
  labs(
    title = "Movies ratings distribution",
    x = "IMDB rating",
    y = "Number of movies", 
  )+

theme(plot.title = element_text(size = 14, face = "bold", hjust=.5), #Finally, we format the table to make it more easily readable.
     axis.text=element_text(size=10), 
     axis.title.x = element_text(size = 12, angle = 0), 
     axis.title.y = element_text(size = 12, angle = 90
  )) +
NULL

histogram_rating
```





## Building data visualizations to derive insights from the data

**Examining the relationship between `gross` and `cast_facebook_likes` with a scatterplot**

There seems a weak positive relationship between the number of cast facebook likes and the gross revenue generated by movies. The correlation coefficient between gross and cast facebook likes is 0.213.

```{r gross_on_fblikes, cache=TRUE} 
movies %>% #We take the relevant dataset.
  ggplot(aes(x = gross, y = cast_facebook_likes, colour = genre)) + #We plot the gross revenue and the number of cast facebook likes of each movie to see the relationship between gross revenue and cast facebook lines. We colour by genre to obtain additional information.
  geom_point() + #We create a scatter plot.
  scale_x_log10() + #We set log scales on the axes of our graph to make it more easily readable.
  scale_y_log10() +
  labs(
    title = "Relationship between gross and cast facebook likes",
    x = "Gross",
    y = "Cast facebook likes"
  )+
#Finally, we format the main title, the axes' titles and the axes' text to make the chart more easily readable.
theme(plot.title = element_text(size = 14, face = "bold", hjust=.5), 
      axis.text=element_text(size=10), 
      axis.title.x = element_text(size = 12, angle = 0), 
      axis.title.y = element_text(size = 12, angle = 90
  )) +
  NULL
z <- cor(movies$gross,movies$cast_facebook_likes) #We compute the correlation between our two variables to quantify their relationship.
z #We print our correlation.
  
```




**Examining the relationship between `gross` and `budget` with a scatterplot**

There seems a strong positive relationship between the number of cast facebook likes and the gross revenue generated by movies. The correlation coefficient between gross and cast facebook likes is 0.213.

```{r gross_on_budget, cache=TRUE}
movies %>% #We take the relevant dataset.
  ggplot(aes(x = gross, y = budget, colour = genre)) + #We plot the gross revenue and the budget of each movie to see the relationship between gross revenue and budget. We colour by genre to obtain additional information.
  geom_point() + #We create a scatterplot.
  scale_x_log10() + #We set log scales on the axes of our graph to make it more easily readable.
  scale_y_log10() +
  labs(
    title = "Relationship between gross and budget",
    y = "Gross",
    x = "Budget"
  )+
#Finally, we format the main title, the axes' titles and the axes' text to make the chart more easily readable.
theme(plot.title = element_text(size = 14, face = "bold", hjust=.5), 
      axis.text=element_text(size=10), 
      axis.title.x = element_text(size = 12, angle = 0), 
      axis.title.y = element_text(size = 12, angle = 90
  )) +
  NULL
z <- cor(movies$gross,movies$budget) #We compute the correlation between our two variables to quantify their relationship.
z #We print our correlation.
```
  



**Examining the relationship between `gross` and `rating` using a scatterplot faceted by `genre`**

There seems a strong positive relationship between the number of cast facebook likes and the gross revenue generated by movies. The correlation coefficient between gross and cast facebook likes is 0.269. It may seem strange that gross and ratings aren't strongly correlated.

```{r gross_on_rating, cache=TRUE}
movies %>% #We take the relevant dataset.
  ggplot(aes(y = gross, x = rating, colour = genre)) + #We plot the gross revenue and the rating of each movie to see the relationship between gross revenue and budget. We colour by genre to obtain additional information.
  geom_point() + #We create a scatterplot.
  scale_x_log10() + #We set log scales on the axes of our graph to make it more easily readable.
  scale_y_log10() +
  labs(
    title = "Relationship between gross and rating",
    y = "Gross",
    x = "IMDB Rating"
  )+
  #Finally, we format the main title, the axes' titles and the axes' text to make the chart more easily readable.
theme(plot.title = element_text(size = 14, face = "bold", hjust=.5), 
      axis.text=element_text(size=10), 
      axis.title.x = element_text(size = 12, angle = 0), 
      axis.title.y = element_text(size = 12, angle = 90
  )) +
  NULL
z <- cor(movies$gross,movies$rating)
z #We print our correlation.
```



---



# Returns of financial stocks

We start off by downloading stock data from the New York Stock Exchange:
```{r load_nyse_data, message=FALSE, warning=FALSE, cache=TRUE}
nyse <- read_csv(here::here("data","nyse.csv"))
```

Then we create a table and a bar plot that shows the number of companies per sector, in descending order:

```{r companies_per_sector, cache=TRUE}
nyse %>% #We take the relevant dataset.
  group_by(sector) %>% #We group the companies listed on the NYSE by sector.
  summarise(sector_count = count(sector)) %>% #We summarise to count the number of companies by sector.
  arrange(desc(sector_count)) %>% #We arrange each sector by the number of companies in a descending order.
  kable() %>% #We format our table to make it more easily readable.
  kable_styling()

nyse %>% #We take the relevant dataset.
  group_by(sector) %>% #We group the companies listed on the NYSE by sector.
  summarise(sector_count = count(sector)) %>% #We summarise to count the number of companies by sector.
  ggplot(aes(x = sector_count, y = fct_reorder(sector, sector_count))) + #We plot the number of listed companies on the NYSE by sector, and colour and fill by sector to make the information more easily readable.
  geom_col()+ #We create a column graph.
  theme_bw()+
  labs(
    title = "Number of companies by sector listed on the NYSE",
    x = "Number of companies",
    y = NULL
  )+
#Finally, we format the main title, the axes' titles and the axes' text to make the chart more easily readable.
theme(plot.title = element_text(size = 14, face = "bold", hjust=.5), 
      axis.text=element_text(size=10), 
      axis.title.x = element_text(size = 12, angle = 0)) +
  NULL

```


We then use the [Dow Jones Industrial Aveareg (DJIA)](https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average) Wikipedia website to find the stocks comprising the index and their ticker symbols. We use this to and download stock data. In addition to the thirty stocks that make up the DJIA, we will also add `SPY` which is an SP500 ETF (Exchange Traded Fund).


```{r tickers_from_wikipedia, cache=TRUE}

djia_url <- "https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average"


#get tables that exist on URL
tables <- djia_url %>% 
  read_html() %>% 
  html_nodes(css="table")


# parse HTML tables into a dataframe called djia. 
# Use purr::map() to create a list of all tables in URL
djia <- map(tables, . %>% 
               html_table(fill=TRUE)%>% 
               clean_names())


# constituents
table1 <- djia[[2]] %>% # the second table on the page contains the ticker symbols
  mutate(date_added = ymd(date_added),
         
         # if a stock is listed on NYSE, its symbol is, e.g., NYSE: MMM
         # We will get prices from yahoo finance which requires just the ticker
         
         # if symbol contains "NYSE*", the * being a wildcard
         # then we jsut drop the first 6 characters in that string
         ticker = ifelse(str_detect(symbol, "NYSE*"),
                          str_sub(symbol,7,11),
                          symbol)
         )

# we need a vector of strings with just the 30 tickers + SPY
tickers <- table1 %>% 
  select(ticker) %>% 
  pull() %>% # pull() gets them as a sting of characters
  c("SPY") # and lets us add SPY, the SP500 ETF

```

Downloading prices for all 30 DJIA consituents and the SPY ETF that tracks SP500 since January 1, 2020

```{r get_price_data, message=FALSE, warning=FALSE, cache=TRUE}
# Notice the cache=TRUE argument in the chunk options. Because getting data is time consuming, # cache=TRUE means that once it downloads data, the chunk will not run again next time you knit your Rmd

myStocks <- tickers %>% 
  tq_get(get  = "stock.prices",
         from = "2020-01-01",
         to   = Sys.Date()) %>% # Sys.Date() returns today's price
  group_by(symbol) 

glimpse(myStocks) # examine the structure of the resulting data frame
```

Financial performance analysis depend on returns, so our first step is to calculate daily and monthly returns.


```{r calculate_returns, message=FALSE, warning=FALSE, cache=TRUE}
#calculate daily returns
myStocks_returns_daily <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "daily", 
               type       = "log",
               col_rename = "daily_returns",
               cols = c(nested.col))  

#calculate monthly  returns
myStocks_returns_monthly <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               type       = "arithmetic",
               col_rename = "monthly_returns",
               cols = c(nested.col)) 

#calculate yearly returns
myStocks_returns_annual <- myStocks %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "yearly", 
               type       = "arithmetic",
               col_rename = "yearly_returns",
               cols = c(nested.col))
```




**Creating a table summarizing monthly returns for each of the stocks and `SPY`**

```{r summarise_monthly_returns, cache=TRUE}
myStocks_returns_monthly %>% #We take the relevant dataset.
  group_by(symbol) %>% #We group the stocks by symbol.
  summarise(min=min(monthly_returns),max=max(monthly_returns),median=median(monthly_returns),mean=mean(monthly_returns),sd=sd(monthly_returns)) %>% #We summarise to obtain the minimum, maximum, median, mean and standard deviation of monthly returns.
  arrange(desc(mean)) %>% #We arrange the symbols by mean monthly returns.
#Finally, we format the main title, the axes' titles and the axes' text to make the chart more easily readable.
  kable() %>% 
  kable_styling()

```



## Density plot for each stock

**Creating a density plot, using `geom_density()`, for each of the stocks**
```{r density_monthly_returns, cache=TRUE}
myStocks_returns_monthly %>% #We take the relevant dataset.
  group_by(symbol) %>% #We group the stocks by symbol.
  ggplot(aes(x = monthly_returns)) + #We define the x-axis of our density plots.
  geom_density() + #We create our density plots.
  facet_wrap(vars(symbol)) #We create a density plot for each stock symbol.

myStocks_returns_monthly %>% #We take the relevant dataset.
  group_by(symbol) %>% #We group the stocks by symbol.
  summarise(sd=sd(monthly_returns),mean = mean(monthly_returns),median = median (monthly_returns)) %>% #We summarise to obtain the median, mean and standard deviation of monthly returns.
  arrange(desc(sd)) %>% #We arrange stock symbols by descending order of SD of monthly returns to see which are the most and less risky ones.
#Finally, we format the charts to make them more easily readable.
  kable() %>% 
  kable_styling()
```



**What can we infer from this plot and which stock is the riskiest/least risky?**

From this plot and a table ranking the stock by the standard deviation of their monthly returns, we can see that the stock BA is the riskiest, whereas the stock VZ is the least risky.



## Plotting expected return and standard deviation

**Plot that shows the expected monthly return (mean) of a stock on the Y axis and the risk (standard deviation) in the X-axis**

```{r risk_return_plot, cache=TRUE}
# YOUR CODE GOES HERE
myStocks_returns_monthly %>%
group_by(symbol) %>%
summarise(M = mean(monthly_returns), S = sd(monthly_returns), color = symbol, label = symbol) %>%
ggplot(aes(x = S, y = M, label = symbol)) +
geom_text_repel(label=myStocks_returns_monthly$symbol,point.padding = NA,size=2,label.padding=0.05, segment.size=0.3)+
geom_point() +
theme_bw()+
labs(
  title = "Expected monthly return and risk proxy of each stock",
  x = "Standard deviation",
  y = "Expected return"
  )+
#Finally, we format the main title, the axes' titles and the axes' text to make the chart more easily readable.
theme(plot.title = element_text(size = 14, face = "bold", hjust = .5), 
      axis.text=element_text(size=10), 
      axis.title.x = element_text(size = 12, angle = 0), 
      axis.title.y = element_text(size = 12, angle = 90
  )) +
NULL
```

**What can you we from this plot? Are there any stocks which, while being riskier, do not have a higher expected return?**

The graph plots each stock's standard deviations against their expected returns. The stocks in the fourth quadrant (bottom right) of the graph have both the highest SD, hence the highest beta, and the lowest historic returns. We would expect these stocks to be poor investments, as they are those that bear the highest risk and the lowest expected returns.



---



# Is inflation transitory?
We want to re-product the following chart:

```{r cpi_10year, echo=FALSE, out.width="90%", cache=TRUE}
knitr::include_graphics(here::here("images", "cpi_10year.png"), error = FALSE)
```



**The right data set**

First we download data, clean, and merge data for CPI and the 10 year bill

```{r data_cpi_10Year_yield, cache=TRUE}
cpi  <-   tq_get("CPIAUCSL", get = "economic.data",
                       from = "1980-01-01") %>% 
  rename(cpi = symbol,  # FRED data is given as 'symbol' and 'price'
         rate = price) %>% # we rename them to what they really are, e.g., cpi and rate
  
  # calculate yearly change in CPI by dividing current month by same month a year (or 12 months) earlier, minus 1
  mutate(cpi_yoy_change = rate/lag(rate, 12) - 1)

ten_year_monthly  <-   tq_get("GS10", get = "economic.data",
                       from = "1980-01-01") %>% 
  rename(ten_year = symbol,
         yield = price) %>% 
  mutate(yield = yield / 100) # original data is not given as, e.g., 0.05, but rather 5, for five percent


# we have the two dataframes-- we now need to join them, and we will use left_join()
mydata <- 
  cpi %>% 
  left_join(ten_year_monthly, by="date") %>% 
  mutate(
    year = year(date), # using lubridate::year() to generate a new column with just the year
    month = month(date, label = TRUE),
    decade=case_when(
      year %in% 1980:1989 ~ "1980s",
      year %in% 1990:1999 ~ "1990s",
      year %in% 2000:2009 ~ "2000s",
      year %in% 2010:2019 ~ "2010s",
      TRUE ~ "2020s"
      )
  )
```



**Building the right graph**

Then we build the graph

```{r chart_cpi_10Year_yield, cache=TRUE}
library(ggrepel)
mydata1 <- mydata%>%
 unite(month_year,month:year,remove = FALSE) 

p<-ggplot(mydata1,aes(x=cpi_yoy_change,y=yield,colour=decade,label= month_year),size=2)

p1<-p+
  geom_point(show.legend=FALSE,size=0.8)+
  facet_wrap(vars(decade),ncol=1,scales="free")+
  labs(title="How are CPI and 10-year related?",x="CPI Yearly change",y="10-Year Treasury Constant Maturity Rate",size=2,caption="Data Source: FRED")+
  geom_smooth(method="lm",show.legend=FALSE,size=0.5, se=FALSE)+
  geom_text_repel(label=mydata1$month_year,point.padding = NA,size=3.5,label.padding=0.05, segment.size=0.3)+
  theme_bw()+
  theme(legend.position = "none",legend.background=element_blank(),plot.title=element_text(size=12,face="bold"),axis.text=element_text(size=7),strip.text=element_text(size=7),axis.title=element_text(size=10,face="bold"),aspect.ratio=1377/2560/5)+
  scale_y_continuous(labels = scales::percent)+
  scale_x_continuous(labels = scales::percent)+
#Finally, we format the main title, the axes' titles and the axes' text to make the chart more easily readable.
theme(plot.title = element_text(size = 20, face = "bold", hjust=.5), 
      axis.text=element_text(size=12), 
      axis.title.x = element_text(size = 20, angle = 0), 
      axis.title.y = element_text(size = 20, angle = 90
  )) +
  NULL
p1
```



---



# Challenge 1: Replicating a chart

In Challenge 1, we are replicating a chart from the following article: [The Racial Factor: There's 77 Counties Which Are Deep Blue But Also Low-Vaxx. Guess What They Have In Common?](https://acasignups.net/21/07/18/racial-factor-theres-77-counties-which-are-deep-blue-also-low-vaxx-guess-what-they-have).

```{r challenge1, echo=FALSE, out.width="90%"}
knitr::include_graphics(here::here("images", "vaxxes_by_state_red_blue_every_county_070321_1.jpg"), error = FALSE)
```



**The data**

Since the article is from july, and we now have access to newer data, we will be using the latest data to create the chart. We will get vaccination data by county from the [CDC](https://data.cdc.gov/Vaccinations/COVID-19-Vaccinations-in-the-United-States-County/8xkx-amqh). Additionally, we use Dataverse from Harvard to get[County Presidential Election Returns 2000-2020](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/VOQCHQ) data. The last data set we need is an estimate of the [population of each county](https://www.ers.usda.gov/webdocs/DataFiles/48747/PopulationEstimates.csv?v=2232) from the Economic Research Service of the United States Department of Agriculture.
 

```{r downloading_challenge_1, echo=FALSE, cache=TRUE}
# Download CDC vaccination by county
#Issues with data timing out - ref. slack with professor. This is our original code and we saved to a csv to avoid time-out issues when knitting:
#cdc_url <- "https://data.cdc.gov/api/views/8xkx-amqh/rows.csv?accessType=DOWNLOAD"
#vaccinations <- vroom(cdc_url) %>% 
  #janitor::clean_names() %>% 
  #filter(fips != "UNK") # remove counties that have an unknown (UNK) FIPS code
#write_csv(vaccinations, "vaccinations.csv")
vaccinations <- read.csv("~/Data Analytics for Finance/ca09.mfa2022/vaccinations.csv")

# Download County Presidential Election Returns
# https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/VOQCHQ
election2020_results <- vroom(here::here("data", "countypres_2000-2020.csv")) %>% 
  janitor::clean_names() %>% 
  
  # just keep the results for the 2020 election
  filter(year == "2020") %>% 
  
  # change original name county_fips to fips, to be consistent with the other two files
  rename (fips = county_fips)

# Download county population data
population_url <- "https://www.ers.usda.gov/webdocs/DataFiles/48747/PopulationEstimates.csv?v=2232"
population <- vroom(population_url) %>% 
  janitor::clean_names() %>% 
  
  # select the latest data, namely 2019
  select(fips = fip_stxt, pop_estimate_2019) %>% 
  
  # pad FIPS codes with leading zeros, so they are always made up of 5 characters
  mutate(fips = stringi::stri_pad_left(fips, width=5, pad = "0"))

```




**Consideration of the data and merging**

Upon inspecting the data we see two major problems with the data: 

1. Some counties have 0% vaccination. Primarilly applies to counties in Texas, Hawaii, and some smaller counties, e.g. in California and Massachusetts.
2. Some counties lack voting data or have 0 votes for each candidate. Primarily smaller counties in 7 states. 

We decide to investigate this further and find the following reasons for the problems with our data set:

1. CDC list [Exceptions to County-Level Data](https://www.cdc.gov/coronavirus/2019-ncov/vaccines/distributing/reporting-counties.html) due to inconsistencies reporting from state to state. California and Massachusetts do not report county level data for smaller counties and Texas only reports state level aggregate data. The inconsistencies we see in our data align with CDCs exceptions explanation. We will, therefore, remove these counties from our final data set.  
2. Seven US states only report congressional district level returns for presidential elections, instead of at the county level. Some counties are congressional districts on their own and these counties have voting data. However, the counties that do not make up congressional districts on their own, but are a part of congressional districts consisting of multiple counties, are missing data. We will, therefore, remove these counties from our final data set. 


```{r merging_inspecting_challenge_1, echo=FALSE, cache=TRUE}

## Merge three datasets into a new one
merge1 <- merge(population,vaccinations,by="fips")
merged_data <- merge(merge1,election2020_results,by="fips")

## Then cleaning data by keeping latest data collected on September 22 and filter data only with candidate as Trump and collected under the mode of "total" and "election day"
final_data<- filter(merged_data, date=="09/22/2021",party=="REPUBLICAN",mode=="TOTAL"|mode=="ELECTION DAY")

## Removing country's with 0 candidatevotes. 
###They are a part of a congressional district in one of the seven states that county votes and assign seats based on congressional districts. Some disctricts consist of mulitple counties, which is why they show 0 candidatevotes. 

final_data<- subset(final_data,candidatevotes>0&series_complete_pop_pct>0)
final_data<- mutate(final_data,votes_per=candidatevotes/totalvotes)


final_data %>%
  filter(series_complete_yes==0,state==c("CALIFORNIA","VIRGINIA")) %>%
  group_by(recip_county) %>%
  summarize(state,pop_estimate_2019,series_complete_pop_pct,series_complete_yes,administered_dose1_pop_pct)%>%
  arrange(desc(pop_estimate_2019))

final_data %>%
  filter(candidatevotes==0) %>%
  group_by(recip_county) %>%
  summarize(state,pop_estimate_2019,candidatevotes, votes_per, series_complete_pop_pct)%>%
  arrange(desc(pop_estimate_2019))

final_data %>%
  filter(recip_state=="UT") %>%
  group_by(recip_county) %>%
  summarize(state,pop_estimate_2019,candidatevotes, votes_per, series_complete_pop_pct)%>%
  arrange(desc(candidatevotes))

final_data %>%
 summarise(mean_vaxx=mean(series_complete_pop_pct),median_vaxx=median(series_complete_pop_pct),sd_vaxx=sd(series_complete_pop_pct))
```




**Building the chart**

Lastly, we go on to replicate the chart.

```{r creating_chart_challenge_1, echo=FALSE, cache=TRUE}
aspect_ratio<-1.5

vaxx<-ggplot(final_data,(aes(x=votes_per,y=series_complete_pop_pct/100)))
vaxx<-vaxx+
  geom_point(mapping=(aes(x=votes_per,y=series_complete_pop_pct/100,size=pop_estimate_2019^(1/3),alpha=-log(pop_estimate_2019)/10)),show.legend=FALSE)+
  geom_smooth(method="lm",show.legend=FALSE,size=0.5, se=FALSE,linetype="dotted",weight=10)+
  geom_hline(aes(yintercept=0.4),size=0.5)+
  geom_hline(aes(yintercept=0.6),size=0.5)+
  geom_vline(aes(xintercept=0.4),size=0.5)+
  geom_vline(aes(xintercept=0.6),size=0.5)+
  theme_bw()+
  theme(legend.position = "none",
        legend.background=element_blank(),
        plot.title=element_text(size=15,face="bold",hjust=0.5),
        axis.ticks=element_blank(),
        axis.text=element_text(size=6),
        strip.text=element_text(size=16),
        axis.title=element_text(size=10,face="bold"),
        plot.subtitle=element_text(size=13,face="bold",hjust=0.5),
        panel.grid.minor = element_blank())+
  coord_fixed(ratio=1.2)+
  labs(title="COVID-19 VACCINATION LEVELS OUT OF TOTAL POPULATION BY COUNTY",
       subtitle="(most states based on FULLY vaccinated only:CA,GA,IA,MI&TX based on total doses administered
       Data via Centers for Disease Control, COVID Act Now,state health depts 
       Graph by Charles Gaba/ ACASignups.net",
       x="2020 Trump Vote %",
       y="% of Total Population Vaccinated")+
  annotate("text",x=0.5,y=0.98,label="EVERY U.S. COUNTY",fontface="bold",size=6)+
  annotate("text",x=0.2,y=0.05,label="y=-0.05565x+0.43510
           R^2=0.0094",fontface="bold",size=4,colour="red")+
  scale_y_continuous(expand = c(0, 0), breaks= seq(0,1,by=0.05),labels = scales::percent_format(accuracy =1),limits=c(0,1))+
  scale_x_continuous(expand = c(0, 0),breaks= seq(0,1,by=0.05),labels = scales::percent_format(accuracy  = 1),limits=c(0,1))+
  NULL

# Run the linear regression line
vaxx.trumpvote.lm <- lm(series_complete_pop_pct/100 ~votes_per , data = final_data)
summary(vaxx.trumpvote.lm)
vaxx
```



---



# Challenge 2: Opinion polls for the 2021 German elections

The Guardian newspaper has an [election poll tracker for the upcoming German election](https://www.theguardian.com/world/2021/aug/20/german-election-poll-tracker-who-will-be-the-next-chancellor). The list of the opinion polls since Jan 2021 can be found at [Wikipedia](https://en.wikipedia.org/wiki/Opinion_polling_for_the_2021_German_federal_election) and we will reproduce a graph similar to the one produced by the Guardian using the data from the wikipedia page . 




**Scraping Wikipedia page**

First, we scrape the wikipedia page and import the table in a dataframe.


```{r scrape_wikipedia_polling_data, warnings= FALSE, message=FALSE, cache=TRUE}
url <- "https://en.wikipedia.org/wiki/Opinion_polling_for_the_2021_German_federal_election"

# similar graphs and analyses can be found at 
# https://www.theguardian.com/world/2021/jun/21/german-election-poll-tracker-who-will-be-the-next-chancellor
# https://www.economist.com/graphic-detail/who-will-succeed-angela-merkel


# get tables that exist on wikipedia page 
tables <- url %>% 
  read_html() %>% 
  html_nodes(css="table")


# parse HTML tables into a dataframe called polls 
# Use purr::map() to create a list of all tables in URL
polls <- map(tables, . %>% 
             html_table(fill=TRUE)%>% 
             janitor::clean_names())


# list of opinion polls
german_election_polls <- polls[[1]] %>% # the first table on the page contains the list of all opinions polls
  slice(2:(n()-1)) %>%  # drop the first row, as it contains again the variable names and last row that contains 2017 results
  mutate(
         # polls are shown to run from-to, e.g. 9-13 Aug 2021. We keep the last date, 13 Aug here, as the poll date
         # and we extract it by picking the last 11 characters from that field
         end_date = str_sub(fieldwork_date, -11),
         
         # end_date is still a string, so we convert it into a date object using lubridate::dmy()
         end_date = dmy(end_date),
         
         # we also get the month and week number from the date, if we want to do analysis by month- week, etc.
         month = month(end_date),
         week = isoweek(end_date)
         )
```




**Building Chart**

Then we reproduce the chart with the new data.

```{r making_challenge_2_plot, warnings= FALSE, message=FALSE, cache=TRUE}
library(plotly)
#To be able to plot all the German parties' polling scores on the y axis by date, we need to rework our data set so as to have the date as a function of all the parties' polling score. To to this, we first have to define variables for each party' polling score.
SPD <- german_election_polls$spd
Union <- german_election_polls$union
AfD <- german_election_polls$af_d
FDP <- german_election_polls$fdp
Linke <- german_election_polls$linke
Grune <- german_election_polls$grune
day <- german_election_polls$end_date

df1 <- data.frame(SPD, Union, AfD, FDP, Linke, Grune, day) #We create a new data frame to which we assign all the variables we created.
df2 <- melt(df1, id.vars='day') #Then, we melt all the parties' polling scores and keep the date as the other variable.
head(df2)

ggplot(df2,aes(x = day, y = value, colour = variable)) + #We plot the days and the parties' polling scores. We colour the parties to be able to differentiate them.
  geom_point(shape=1) + #We make the points hollow to be able to better see the rolling average we'll create.
  geom_ma(spd_MA= SMA, n=14, linetype = 1, size=1) + #We create a 14-day rolling average following the Guardian's methodology. We change the linetype to a solid one so that it is more easily readable.
theme_bw()+
labs(
  title = "Polling scores for the German election",
  x = "Day (2021)",
  y = "German parties' polling scores",
)+
#Finally, we format the main title, the axes' titles and the axes' text to make the chart more easily readable.
theme(plot.title = element_text(size = 14, face = "bold")) +
theme(axis.title.x = element_text(size = 12, angle = 0)) +
theme(axis.title.y = element_text(size = 12, angle = 90)) +
  NULL
```



--- 



# Details

- Who did you collaborate with: Only our study group B9. 

![](https://drive.google.com/uc?id=1PgzxOqzleAl99qYRTIMTBgNUnylb1rIb) 

*The best study group B9! From the left: Keshav, Gianmaria, Hadrien, Jenny (Metavee), and Sherington. Our last group member Cissie is unfortunately missing from the picture.*

- Approximately how much time did you spend on this problem set: 30 hours
- What, if anything, gave you the most trouble: the challenges were challenging (obviously) because we had to google a lot of error messages to find the right formulas and packages that could be used to create the tables and graphs we wanted.
